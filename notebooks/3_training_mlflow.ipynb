{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4e9f4d",
   "metadata": {},
   "source": [
    "# 3_training_mlflow.ipynb\n",
    "\n",
    "- **Objectif** :\n",
    "Lancer et suivre les entra√Ænements des mod√®les ML (Logistic, Random Forest, XGBoost)\n",
    "avec diff√©rentes strat√©gies de d√©s√©quilibre, via MLflow.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc36db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079cb40",
   "metadata": {},
   "source": [
    "## D√©finir le dossier du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c0b9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir le dossier du projet\n",
    "os.chdir(\"..\") if \"notebooks\" in os.getcwd() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8a270",
   "metadata": {},
   "source": [
    "## Configurer le tracking local MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dfe9866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/PC/Desktop/sprint5/Churn-Prediction-avec-MLflow/mlruns/667014208243130233', creation_time=1761207273202, experiment_id='667014208243130233', last_update_time=1761207273202, lifecycle_stage='active', name='Churn_Prediction_Models', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurer le tracking local MLflow\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"Churn_Prediction_Models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a71d137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üöÄ Lancement des entra√Ænements et suivi MLflow"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"## üöÄ Lancement des entra√Ænements et suivi MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742c336",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Lancer les scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3873ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 1Ô∏è‚É£ Lancer les scripts\n",
    "# ==========================================================\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3660d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = [\n",
    "    \"src/models/logistic.py\",\n",
    "    \"src/models/forest.py\",\n",
    "    \"src/models/xgboost.py\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d7d584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Entra√Ænement avec src/models/logistic.py ...\n",
      "‚úÖ Termin√© : src/models/logistic.py\n",
      "‚öôÔ∏è Entra√Ænement avec src/models/forest.py ...\n",
      "‚úÖ Termin√© : src/models/forest.py\n",
      "‚öôÔ∏è Entra√Ænement avec src/models/xgboost.py ...\n",
      "‚úÖ Termin√© : src/models/xgboost.py\n"
     ]
    }
   ],
   "source": [
    "for script in scripts:\n",
    "    print(f\"‚öôÔ∏è Entra√Ænement avec {script} ...\")\n",
    "    subprocess.run([\"python\", script])\n",
    "    print(f\"‚úÖ Termin√© : {script}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16b8b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time', 'end_time', 'metrics.accuracy', 'metrics.f1', 'params.note', 'tags.mlflow.runName', 'tags.mlflow.source.git.commit', 'tags.mlflow.source.type', 'tags.mlflow.user', 'tags.mlflow.source.name', 'tags.model_name', 'tags.imbalance_strategy']\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# R√©cup√©rer l'ID de l'exp√©rience\n",
    "experiment = mlflow.get_experiment_by_name(\"Churn_Prediction_Models\")\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Chercher tous les runs de l'exp√©rience\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], filter_string=\"\", order_by=[\"start_time DESC\"])\n",
    "print(runs.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a651a",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Comparaison des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9be2f5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üìä Visualisation dans MLflow UI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 2Ô∏è‚É£ Comparaison des r√©sultats\n",
    "# ==========================================================\n",
    "md(\"### üìä Visualisation dans MLflow UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47886e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pour ouvrir l'interface MLflow :\n",
      "> mlflow ui --backend-store-uri ./mlruns\n",
      "Puis ouvrir : http://127.0.0.1:5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Pour ouvrir l'interface MLflow :\n",
    "> mlflow ui --backend-store-uri ./mlruns\n",
    "Puis ouvrir : http://127.0.0.1:5000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14cb0429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üèÜ S√©lection du meilleur mod√®le"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"### üèÜ S√©lection du meilleur mod√®le\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aabc40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e839f3",
   "metadata": {},
   "source": [
    "## Charger les runs MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "369da389",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m runs_info = \u001b[43mclient\u001b[49m.search_runs(experiment_ids=[experiment.experiment_id])\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(runs_info)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m runs trouv√©s\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Cr√©er le DataFrame\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "runs_info = client.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "print(f\"‚úÖ {len(runs_info)} runs trouv√©s\")\n",
    "\n",
    "# Cr√©er le DataFrame\n",
    "runs_data = []\n",
    "for run in runs_info:\n",
    "    run_dict = {\n",
    "        'run_id': run.info.run_id,\n",
    "        'tags.model_name': run.data.tags.get('model_name', 'Unknown'),\n",
    "        'tags.imbalance_strategy': run.data.tags.get('imbalance_strategy', 'Unknown'),\n",
    "        'metrics.f1_score': run.data.metrics.get('f1_score', 0),\n",
    "        'metrics.accuracy': run.data.metrics.get('accuracy', 0)\n",
    "    }\n",
    "    runs_data.append(run_dict)\n",
    "runs = pd.DataFrame(runs_data)\n",
    "\n",
    "# V√©rifier les colonnes disponibles\n",
    "print(\"üìã Colonnes disponibles :\", runs.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304da8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Colonnes disponibles : ['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time', 'end_time', 'metrics.accuracy', 'metrics.f1', 'params.note', 'tags.mlflow.user', 'tags.mlflow.runName', 'tags.mlflow.source.name', 'tags.mlflow.source.type', 'tags.mlflow.source.git.commit', 'tags.model_name', 'tags.imbalance_strategy']\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier les colonnes disponibles\n",
    "print(\"üìã Colonnes disponibles :\", runs.columns.tolist())\n",
    "\n",
    "# Rechercher la colonne correspondant √† la m√©trique F1\n",
    "f1_cols = [col for col in runs.columns if \"f1_score\" in col.lower() and col.startswith(\"metrics.\")]\n",
    "\n",
    "# Si aucune m√©trique F1 n'est trouv√©e, afficher un message explicatif au lieu de lever une erreur\n",
    "if not f1_cols:\n",
    "    print(\"‚ö†Ô∏è Aucune m√©trique F1 trouv√©e dans les runs MLflow.\")\n",
    "    print(\"üëâ V√©rifiez que la m√©trique F1 a bien √©t√© logg√©e avec `mlflow.log_metric('f1', valeur)`.\")\n",
    "    print(\"üîé Exemple de colonnes disponibles li√©es aux m√©triques :\",\n",
    "          [col for col in runs.columns if col.startswith(\"metrics.\")])\n",
    "else:\n",
    "    # S√©lectionner la premi√®re colonne F1 trouv√©e\n",
    "    f1_col = f1_cols[0]\n",
    "\n",
    "    # Trier les runs selon la meilleure valeur de F1\n",
    "    best_run = runs.sort_values(f1_col, ascending=False).iloc[0]\n",
    "\n",
    "    # Afficher le meilleur mod√®le avec sa valeur F1\n",
    "    md(f\"**üèÜ Meilleur mod√®le :** {best_run.get('tags.model_name', 'Nom inconnu')} \"\n",
    "       f\"({f1_col} = {best_run[f1_col]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "metrics.accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "metrics.f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1bfb0a45-4917-41ff-8bda-f7465ecc7239",
       "rows": [
        [
         "0",
         null,
         null
        ],
        [
         "1",
         "0.5",
         "0.5"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics.accuracy</th>\n",
       "      <th>metrics.f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metrics.accuracy  metrics.f1\n",
       "0               NaN         NaN\n",
       "1               0.5         0.5"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs.filter(like=\"metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa49d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Meilleur mod√®le :** DummyModel (F1 = 0.500)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run = runs.sort_values(\"metrics.f1_score\", ascending=False).iloc[0]\n",
    "md(f\"**Meilleur mod√®le :** {best_run['tags.model_name']} (F1 = {best_run['metrics.f1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c6fb9",
   "metadata": {},
   "source": [
    "## Chargement et test du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f546c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement et test du mod√®le\n",
    "model_uri = f\"runs:/{best_run['run_id']}/model\"\n",
    "try:\n",
    "    loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "    print(\"Mod√®le charg√©\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur chargement mod√®le: {e}\")\n",
    "    loaded_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638439a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"### üîç Test d'une pr√©diction avec le mod√®le charg√©\")\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a368340",
   "metadata": {},
   "source": [
    "## Exemple d‚Äôun batch de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa9d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pr√©diction exemple :\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Exemple d‚Äôun batch de test\n",
    "X_test = joblib.load(\"data/X_test.pkl\")\n",
    "print(\"‚úÖ Pr√©diction exemple :\")\n",
    "print(loaded_model.predict(X_test[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983ddc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
